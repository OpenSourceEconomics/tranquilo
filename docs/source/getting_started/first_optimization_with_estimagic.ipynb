{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical optimization\n",
    "\n",
    "This tutorial shows how to do an optimization with estimagic with simple examples. More details on the topics covered here can be found in the [how to guides](../how_to_guides/index.rst).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage of `minimize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -0., -0., -0., -0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from estimagic import minimize\n",
    "\n",
    "\n",
    "def sphere(params):\n",
    "    return params @ params\n",
    "\n",
    "\n",
    "res = minimize(\n",
    "    criterion=sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    ")\n",
    "\n",
    "res[\"solution_params\"].round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `params` do not have to be vectors\n",
    "\n",
    "In estimagic, params can by arbitrary [pytrees](https://jax.readthedocs.io/en/latest/pytrees.html). Examples are (nested) dictionaries of numbers, arrays and pandas objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 3.885293415171424e-09,\n",
       " 'b': -9.144916295625622e-09,\n",
       " 'c': 0   -6.348281e-09\n",
       " 1   -4.599364e-09\n",
       " 2   -2.724007e-09\n",
       " dtype: float64}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dict_sphere(params):\n",
    "    return params[\"a\"] ** 2 + params[\"b\"] ** 2 + (params[\"c\"] ** 2).sum()\n",
    "\n",
    "\n",
    "res = minimize(\n",
    "    criterion=dict_sphere,\n",
    "    params={\"a\": 0, \"b\": 1, \"c\": pd.Series([2, 3, 4])},\n",
    "    algorithm=\"scipy_neldermead\",\n",
    ")\n",
    "\n",
    "res[\"solution_params\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are many optimizers\n",
    "\n",
    "If you install some optional dependencies, you can choose from a large (and growing) set of optimization algorithms -- all with the same interface!\n",
    "\n",
    "For example, we wrap optimizers from `scipy.optimize`, `nlopt`, `cyipopt`, `pygmo`, `fides`, `tao` and others. \n",
    "\n",
    "We also have some optimizers that are not part of other packages. Examples are a `parallel Nelder-Mead` algorithm, The `BHHH` algorithm and a `parallel Pounders` algorithm.\n",
    "\n",
    "The full list is [here](../how_to_guides/optimization/how_to_specify_algorithm_and_algo_options.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can add bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 2.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = minimize(\n",
    "    criterion=sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    lower_bounds=np.arange(5) - 2,\n",
    "    upper_bounds=np.array([10, 10, 10, np.inf, np.inf]),\n",
    ")\n",
    "\n",
    "res[\"solution_params\"].round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can fix parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 3., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = minimize(\n",
    "    criterion=sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    constraints=[{\"loc\": [1, 3], \"type\": \"fixed\"}],\n",
    ")\n",
    "\n",
    "res[\"solution_params\"].round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or impose other constraints\n",
    "\n",
    "As an example, let's impose the constraint that the first three parameters are valid probabilities, i.e. they are between zero and one and sum to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.33334,  0.33333,  0.33333, -0.     ,  0.     ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = minimize(\n",
    "    criterion=sphere,\n",
    "    params=np.array([0.1, 0.5, 0.4, 4, 5]),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    constraints=[{\"loc\": [0, 1, 2], \"type\": \"probability\"}],\n",
    ")\n",
    "\n",
    "res[\"solution_params\"].round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full overview of the constraints we support and the syntax, check out [the documentation](../how_to_guides/optimization/how_to_specify_constraints.rst).\n",
    "\n",
    "Note that `\"scipy_lbfgsb\"` is not a constrained optimizer. If you want to know how we achieve this, check out [the explanations](../explanations/optimization/implementation_of_constraints.rst)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There is also maximize\n",
    "\n",
    "If you ever forgot to switch back the sign of your criterion function after doing a maximization with `scipy.optimize.minimize`, there is good news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -0., -0.,  0., -0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from estimagic import maximize\n",
    "\n",
    "\n",
    "def upside_down_sphere(params):\n",
    "    return -params @ params\n",
    "\n",
    "\n",
    "res = maximize(\n",
    "    criterion=upside_down_sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_bfgs\",\n",
    ")\n",
    "\n",
    "res[\"solution_params\"].round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can provide closed form derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -0., -0., -0., -0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sphere_gradient(params):\n",
    "    return 2 * params\n",
    "\n",
    "\n",
    "res = minimize(\n",
    "    criterion=sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    derivative=sphere_gradient,\n",
    ")\n",
    "res[\"solution_params\"].round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or use parallelized numerical derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -0., -0., -0., -0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = minimize(\n",
    "    criterion=sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    numdiff_options={\"n_cores\": 6},\n",
    ")\n",
    "\n",
    "res[\"solution_params\"].round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn local optimizers global with multistart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = minimize(\n",
    "    criterion=sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    soft_lower_bounds=np.full(5, -10),\n",
    "    soft_upper_bounds=np.full(5, 10),\n",
    "    multistart=True,\n",
    ")\n",
    "\n",
    "res[\"solution_params\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploit the structure of your optimization problem\n",
    "\n",
    "Many estimation problems have a least-squares structure. If so, specialized optimizers that exploit this structure can be faster than standard optimizers. Other problems have at least a sum-structure that can be exploited by optimizers (e.g. likelihood functions).\n",
    "\n",
    "If you defined your criterion function a bit differently, you can seamlessly switch between least-squares, sum-structure and standard optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0., -0.,  0., -0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def general_sphere(params):\n",
    "    contribs = params**2\n",
    "    out = {\n",
    "        # root_contributions are the least squares residuals.\n",
    "        # if you square and sum them, you get the criterion value\n",
    "        \"root_contributions\": params,\n",
    "        # if you sum up contributions, you get the criterion value\n",
    "        \"contributions\": contribs,\n",
    "        # this is the standard output\n",
    "        \"value\": contribs.sum(),\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "res = minimize(\n",
    "    criterion=general_sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"pounders\",\n",
    ")\n",
    "res[\"solution_params\"].round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using and reading persistent logging\n",
    "\n",
    "For long running and difficult optimizations, it can be good to store the progress in a persistent log file. You can do this providing a path as `logging` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(\n",
    "    criterion=sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    logging=\"my_log.db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read the entries in the log file (while the optimization is still running or afterwards) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from estimagic.logging.read_log import read_optimization_iteration\n",
    "\n",
    "# the second argument works like an index to a list, i.e.\n",
    "# -1 gives the last entry\n",
    "# read_optimization_iteration(\"my_log.db\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The persistent log file is always instantly synchronized when the optimizer tries a new parameter vector. This is very handy if an optimization has to be aborted and you want to extract the current status. It is also used by the  [estimagic dashboard](../how_to_guides/optimization/how_to_use_the_dashboard.rst). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize your optimizer\n",
    "\n",
    "Most algorithms have a few optional arguments. Examples are convergence criteria or tuning parameters. You can find an overview of supported arguments [here](../how_to_guides/optimization/how_to_specify_algorithm_and_algo_options.rst)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -0., -0., -0., -0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo_options = {\n",
    "    \"convergence.relative_criterion_tolerance\": 1e-9,\n",
    "    \"stopping.max_iterations\": 100_000,\n",
    "}\n",
    "\n",
    "res = minimize(\n",
    "    criterion=sphere,\n",
    "    params=np.arange(5),\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    algo_options=algo_options,\n",
    ")\n",
    "res[\"solution_params\"].round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c2651339f23e96386682d840c81116257052bf3276768d94f8c4b872a05d3e0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
