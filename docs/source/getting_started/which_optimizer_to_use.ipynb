{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from estimagic import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which optimizer to use\n",
    "\n",
    "This is the very very very short guide on selecting a suitable optimization algorithm based on a minimum of information. We are working on a longer version that contains more background information and can be found [here](../how_to_guides/optimization/how_to_choose_optimizer.rst). \n",
    "\n",
    "However, we will also keep this short guide for very impatient people who feel lucky enough. \n",
    "\n",
    "To select an optimizer, you need to answer two questions:\n",
    "\n",
    "1. Is your criterion function differentiable?\n",
    "\n",
    "2. Do you have a nonlinear least squares structure (i.e. do you sum some kind of squared residuals at the end of your criterion function)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some inputs\n",
    "\n",
    "Again, we use versions of the sphere function to illustrate how you select these algorithms in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x_0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_3</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     value\n",
       "x_0      1\n",
       "x_1      2\n",
       "x_2      3\n",
       "x_3      4\n",
       "x_4      5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sphere(params):\n",
    "    \"\"\"Spherical criterion function.\n",
    "\n",
    "    The unique local and global optimum of this function is at\n",
    "    the zero vector. It is differentiable, convex and extremely\n",
    "    well behaved in any possible sense.\n",
    "\n",
    "    Args:\n",
    "        params (pandas.DataFrame): DataFrame with the columns\n",
    "            \"value\", \"lower_bound\", \"upper_bound\" and potentially more.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the entries \"value\" and \"root_contributions\".\n",
    "\n",
    "    \"\"\"\n",
    "    out = {\n",
    "        \"value\": (params[\"value\"] ** 2).sum(),\n",
    "        \"root_contributions\": params[\"value\"],\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "def sphere_gradient(params):\n",
    "    \"\"\"Gradient of spherical criterion function\"\"\"\n",
    "    return params[\"value\"] * 2\n",
    "\n",
    "\n",
    "start_params = pd.DataFrame(\n",
    "    data=np.arange(5) + 1,\n",
    "    columns=[\"value\"],\n",
    "    index=[f\"x_{i}\" for i in range(5)],\n",
    ")\n",
    "start_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable criterion function\n",
    "\n",
    "Use `scipy_lbfsgsb` as optimizer and provide the closed form derivative if you can. If you do not provide a derivative, estimagic will calculate it numerically. However, this is less precise and slower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solution_x': array([ 1.11022302e-16,  2.22044605e-16,  0.00000000e+00,  4.44089210e-16,\n",
       "        -8.88178420e-16]),\n",
       " 'solution_criterion': 1.0477058897466563e-30,\n",
       " 'solution_derivative': array([ 2.22044605e-16,  4.44089210e-16,  0.00000000e+00,  8.88178420e-16,\n",
       "        -1.77635684e-15]),\n",
       " 'solution_hessian': None,\n",
       " 'n_criterion_evaluations': 3,\n",
       " 'n_derivative_evaluations': None,\n",
       " 'n_iterations': 2,\n",
       " 'success': True,\n",
       " 'reached_convergence_criterion': None,\n",
       " 'message': b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       " 'solution_params':      lower_bound  upper_bound         value\n",
       " x_0         -inf          inf  1.110223e-16\n",
       " x_1         -inf          inf  2.220446e-16\n",
       " x_2         -inf          inf  0.000000e+00\n",
       " x_3         -inf          inf  4.440892e-16\n",
       " x_4         -inf          inf -8.881784e-16}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(\n",
    "    criterion=sphere,\n",
    "    params=start_params,\n",
    "    algorithm=\"scipy_lbfgsb\",\n",
    "    derivative=sphere_gradient,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this solves a 5 dimensional problem with just 3 criterion evaluations. For higher dimensions it will need more, but it scales very well to dozens and hundreds of parameters. \n",
    "\n",
    "If you are worried about being stuck in a local optimum, start the optimization several times from random start values and take the best solution of all runs. This will still be much faster than using a global optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not differentiable, only scalar output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `nag_pybobyqa`. Note that for this you need to install the `PyBOBYQA` package if you do not already have it:\n",
    "    \n",
    "`pip install Py-BOBYQA`\n",
    "\n",
    "Then you select the algorithm as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solution_criterion': 1.2999837387099211e-43,\n",
       " 'n_criterion_evaluations': 33,\n",
       " 'message': 'Success: rho has reached rhoend',\n",
       " 'success': True,\n",
       " 'reached_convergence_criterion': None,\n",
       " 'solution_x': array([-2.68103994e-22, -7.69644900e-23,  1.87575580e-22, -1.07763474e-22,\n",
       "        -7.34678507e-23]),\n",
       " 'solution_derivative': array([9.99200776e-15, 2.22044606e-14, 1.42108543e-14, 2.15526949e-22,\n",
       "        1.24344980e-14]),\n",
       " 'solution_hessian': array([[ 2.00000000e+00, -2.39216631e-15,  5.40507868e-16,\n",
       "         -4.95351595e-16,  1.78148798e-15],\n",
       "        [-2.39216631e-15,  2.00000000e+00,  1.02427464e-15,\n",
       "         -2.35943190e-15,  4.50984317e-16],\n",
       "        [ 5.40507868e-16,  1.02427464e-15,  2.00000000e+00,\n",
       "         -3.80803391e-15, -7.00163695e-16],\n",
       "        [-4.95351595e-16, -2.35943190e-15, -3.80803391e-15,\n",
       "          2.00000000e+00,  3.87423301e-15],\n",
       "        [ 1.78148798e-15,  4.50984317e-16, -7.00163695e-16,\n",
       "          3.87423301e-15,  2.00000000e+00]]),\n",
       " 'solution_params':      lower_bound  upper_bound         value\n",
       " x_0         -inf          inf -2.681040e-22\n",
       " x_1         -inf          inf -7.696449e-23\n",
       " x_2         -inf          inf  1.875756e-22\n",
       " x_3         -inf          inf -1.077635e-22\n",
       " x_4         -inf          inf -7.346785e-23,\n",
       " 'n_derivative_evaluations': 'Not reported by nag_pybobyqa',\n",
       " 'n_iterations': 'Not reported by nag_pybobyqa'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(criterion=sphere, params=start_params, algorithm=\"nag_pybobyqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not differentiable, least squares structure\n",
    "\n",
    "Use `nag_dfols` or `tao_pounders`. To use `nag_dfols` you need to install it via:\n",
    "\n",
    "`pip install DFO-LS`\n",
    "\n",
    "To use `tao_pounders` you need to install `petsc4py` via:\n",
    "\n",
    "`conda install petsc4py`. \n",
    "\n",
    "Note that `petsc4py` is only available on Linux on Ubuntu. \n",
    "\n",
    "Both optimizers will only work if your criterion function returns a dictionary that contains the entry `root_contributions`. This needs to be a numpy array or pandas.Series that contains the residuals of the least squares problem. \n",
    "\n",
    "`nag_dfols` performs better for noisy criterion functions. `tao_pounders` performs better for deterministic but very nonlinear criterion functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solution_criterion': 9.590576455224451e-28,\n",
       " 'n_criterion_evaluations': 9,\n",
       " 'message': 'Success: Objective is sufficiently small',\n",
       " 'success': True,\n",
       " 'reached_convergence_criterion': None,\n",
       " 'solution_x': array([-1.33226763e-15,  1.99840144e-14, -1.90958360e-14, -6.21724894e-15,\n",
       "        -1.24344979e-14]),\n",
       " 'solution_params':      lower_bound  upper_bound         value\n",
       " x_0         -inf          inf -1.332268e-15\n",
       " x_1         -inf          inf  1.998401e-14\n",
       " x_2         -inf          inf -1.909584e-14\n",
       " x_3         -inf          inf -6.217249e-15\n",
       " x_4         -inf          inf -1.243450e-14,\n",
       " 'solution_derivative': 'Not reported by nag_dfols',\n",
       " 'solution_hessian': 'Not reported by nag_dfols',\n",
       " 'n_derivative_evaluations': 'Not reported by nag_dfols',\n",
       " 'n_iterations': 'Not reported by nag_dfols'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(\n",
    "    criterion=sphere,\n",
    "    params=start_params,\n",
    "    algorithm=\"nag_dfols\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solution_x': array([1.34645209e-14, 1.39723623e-14, 5.99670193e-15, 2.22044605e-16,\n",
       "        2.22044605e-16]),\n",
       " 'solution_criterion': 4.125792718474871e-28,\n",
       " 'solution_derivative': None,\n",
       " 'solution_hessian': None,\n",
       " 'n_criterion_evaluations': 50,\n",
       " 'n_derivative_evaluations': None,\n",
       " 'n_iterations': None,\n",
       " 'success': True,\n",
       " 'reached_convergence_criterion': 'step size small',\n",
       " 'message': 'step size small',\n",
       " 'solution_criterion_values': array([1.34645209e-14, 1.39723623e-14, 5.99670193e-15, 2.22044605e-16,\n",
       "        2.22044605e-16]),\n",
       " 'gradient_norm': 2.031204745582008e-26,\n",
       " 'criterion_norm': 0.0,\n",
       " 'convergence_code': 6,\n",
       " 'solution_params':      lower_bound  upper_bound         value\n",
       " x_0         -inf          inf  1.346452e-14\n",
       " x_1         -inf          inf  1.397236e-14\n",
       " x_2         -inf          inf  5.996702e-15\n",
       " x_3         -inf          inf  2.220446e-16\n",
       " x_4         -inf          inf  2.220446e-16}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(\n",
    "    criterion=sphere,\n",
    "    params=start_params,\n",
    "    algorithm=\"tao_pounders\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
